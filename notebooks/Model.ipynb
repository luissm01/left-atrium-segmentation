{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "horizontal-player",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this notebook we will create the model for the atrium segmentation! <br />\n",
    "We will use the most famous architecture for this task, the U-NET (https://arxiv.org/abs/1505.04597)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-belly",
   "metadata": {},
   "source": [
    "## Imports:\n",
    "1. torch for model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "noticed-richardson",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-rainbow",
   "metadata": {},
   "source": [
    "## What is U-Net?\n",
    "\n",
    "U-Net is a type of deep learning model that was specifically designed for biomedical image segmentation. It allows the model to predict a detailed mask of an object (like an organ) in an image.\n",
    "\n",
    "The structure of U-Net is based on two main ideas: **an Encoder-Decoder architecture** and **skip connections** between the encoder and decoder.\n",
    "\n",
    "- **Encoder**:  \n",
    "  The encoder is the first part of the network. It takes the input image and processes it through several layers. Each time, it uses operations called **convolutions** and **downsampling** to reduce the spatial size (width and height) of the image but to capture more abstract and important features.  \n",
    "  Think of the encoder as compressing the image, keeping only the most essential information.\n",
    "\n",
    "- **Decoder**:  \n",
    "  The decoder is the second part of the network. Its job is the opposite of the encoder: it **reconstructs** the spatial structure by gradually increasing the width and height of the feature maps through **upsampling**. The final output has the same size as the input image, but instead of being an image, it is a **segmentation mask** that shows which pixels belong to the object of interest.\n",
    "\n",
    "- **Skip Connections**:  \n",
    "  A key feature of U-Net are the skip connections. At each level where the encoder compresses the image, a copy of the features is sent directly to the corresponding level of the decoder.  \n",
    "  This helps the decoder recover fine details that would otherwise be lost during downsampling. It also makes the training process much easier and leads to higher-quality segmentation results.\n",
    "\n",
    "In simple terms:\n",
    "- The encoder captures **what** is in the image.\n",
    "- The decoder reconstructs **where** it is in the image.\n",
    "- Skip connections help keep **all the important details**.\n",
    "\n",
    "Thanks to this design, U-Net can produce very accurate segmentation masks even when the input data is limited, which makes it particularly popular for medical imaging tasks where high precision is crucial.\n",
    "\n",
    "![title](../images/unet.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternative-grove",
   "metadata": {},
   "source": [
    "## Convolutions\n",
    "At first, we define a single Convolution block.\n",
    "Two convolutions are used between each down- or upconvolution step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "going-nicholas",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Helper Class which implements the intermediate Convolutions\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.step = torch.nn.Sequential(torch.nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "                                        torch.nn.ReLU(),\n",
    "                                        torch.nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "                                        torch.nn.ReLU())\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.step(X)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turned-campus",
   "metadata": {},
   "source": [
    "## UNET\n",
    "We now define the U-Net model used for left atrium segmentation.  \n",
    "The architecture follows the classic encoderâ€“decoder structure:\n",
    "\n",
    "- The **encoder** consists of repeated `DoubleConv` blocks followed by `MaxPool2d` for downsampling. Each step reduces spatial resolution while increasing the number of feature channels.\n",
    "- The **decoder** mirrors the encoder: it uses `Upsample` to increase spatial resolution and concatenates the corresponding feature maps from the encoder (skip connections) before applying another `DoubleConv`.\n",
    "\n",
    "This structure allows the network to capture both global context and fine details, making it ideal for segmentation tasks.\n",
    "\n",
    "> Note: `Upsample` is used for simplicity, but you may replace it with `ConvTranspose2d` for learnable upsampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "blocked-killer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.layer1 = DoubleConv(1, 64)\n",
    "        self.layer2 = DoubleConv(64, 128)\n",
    "        self.layer3 = DoubleConv(128, 256)\n",
    "        self.layer4 = DoubleConv(256, 512)\n",
    "        \n",
    "        # Decoder (with skip connections)\n",
    "        self.upconv5 = torch.nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.layer5 = DoubleConv(512, 256)\n",
    "\n",
    "        self.upconv6 = torch.nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.layer6 = DoubleConv(256, 128)\n",
    "\n",
    "        self.upconv7 = torch.nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.layer7 = DoubleConv(128, 64)\n",
    "\n",
    "        # Final output layer\n",
    "        self.layer8 = torch.nn.Conv2d(64, 1, kernel_size=1)\n",
    "        \n",
    "        self.maxpool = torch.nn.MaxPool2d(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder path\n",
    "        x1 = self.layer1(x)\n",
    "        x2 = self.layer2(self.maxpool(x1))\n",
    "        x3 = self.layer3(self.maxpool(x2))\n",
    "        x4 = self.layer4(self.maxpool(x3))\n",
    "\n",
    "        # Decoder path with skip connections\n",
    "        x5 = self.upconv5(x4)\n",
    "        x5 = torch.cat([x5, x3], dim=1)\n",
    "        x5 = self.layer5(x5)\n",
    "\n",
    "        x6 = self.upconv6(x5)\n",
    "        x6 = torch.cat([x6, x2], dim=1)\n",
    "        x6 = self.layer6(x6)\n",
    "\n",
    "        x7 = self.upconv7(x6)\n",
    "        x7 = torch.cat([x7, x1], dim=1)\n",
    "        x7 = self.layer7(x7)\n",
    "\n",
    "        # Final output\n",
    "        return self.layer8(x7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removable-plumbing",
   "metadata": {},
   "source": [
    "## Testing\n",
    "Before training, we perform a simple test to verify that the U-Net architecture works as expected.  \n",
    "We generate a random input tensor of shape `(1, 1, 256, 256)`, simulating a single grayscale image, and pass it through the model.\n",
    "\n",
    "The output is expected to have the same spatial dimensions as the input, but with one output channel representing the segmentation mask.\n",
    "\n",
    "If the output shape matches, this confirms that:\n",
    "- The encoder and decoder paths are correctly balanced.\n",
    "- The `ConvTranspose2d` operations restore the resolution properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ongoing-cattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "grand-samuel",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_input = torch.randn(1, 1, 256, 256)\n",
    "output = model(random_input)\n",
    "assert output.shape == torch.Size([1, 1, 256, 256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-share",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
